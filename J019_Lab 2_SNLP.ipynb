{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score \n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "wv_pretrained = api.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593831062317),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_pretrained.most_similar(positive=[\"king\",\"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy Results:\n",
      "king - man + woman ≈ queen\n",
      "('queen', 0.7118193507194519)\n",
      "brother - man + woman ≈ sister\n",
      "('sister', 0.8103213906288147)\n",
      "uncle - man + woman ≈ aunt\n",
      "('aunt', 0.8022665977478027)\n",
      "nephew - man + woman ≈ niece\n",
      "('niece', 0.8202236890792847)\n",
      "actor - man + woman ≈ actress\n",
      "('actress', 0.8602624535560608)\n",
      "hero - man + woman ≈ heroine\n",
      "('heroine', 0.68734210729599)\n",
      "doctor - hospital + school ≈ guidance_counselor\n",
      "('guidance_counselor', 0.5969595313072205)\n",
      "painter - canvas + stage ≈ cabaret_performer\n",
      "('cabaret_performer', 0.4397791624069214)\n",
      "pilot - airplane + ship ≈ ships\n",
      "('ships', 0.4948738217353821)\n",
      "chef - kitchen + laboratory ≈ lab\n",
      "('lab', 0.5366887450218201)\n",
      "author - book + song ≈ anthem\n",
      "('anthem', 0.5882687568664551)\n",
      "hitler - Germany + India ≈ gandhi\n",
      "('gandhi', 0.6194782853126526)\n"
     ]
    }
   ],
   "source": [
    "def perform_analogy(word1, word2, word3):\n",
    "    result = wv_pretrained.most_similar(positive=[word1, word3], negative=[word2])\n",
    "    return result[0]\n",
    "\n",
    "analogies = [\n",
    "    (\"king\", \"man\", \"woman\"),  \n",
    "    (\"brother\", \"man\", \"woman\"),  \n",
    "    (\"uncle\", \"man\", \"woman\"),  \n",
    "    (\"nephew\", \"man\", \"woman\"),  \n",
    "    (\"actor\", \"man\", \"woman\"),  \n",
    "    (\"hero\", \"man\", \"woman\") ,\n",
    "    (\"doctor\", \"hospital\", \"school\"),  # doctor - hospital + school ≈ teacher\n",
    "    (\"painter\", \"canvas\", \"stage\"),  # painter - canvas + stage ≈ actor\n",
    "    (\"pilot\", \"airplane\", \"ship\"),  # pilot - airplane + ship ≈ captain\n",
    "    (\"chef\", \"kitchen\", \"laboratory\"),  # chef - kitchen + laboratory ≈ scientist\n",
    "    (\"author\", \"book\", \"song\"),  # author - book + song ≈ singer\n",
    "    (\"hitler\",\"Germany\", \"India\")\n",
    "]\n",
    "\n",
    "analogy_results = {analogy: perform_analogy(*analogy) for analogy in analogies}\n",
    "\n",
    "print(\"Analogy Results:\")\n",
    "for analogy, result in analogy_results.items():\n",
    "    print(f\"{analogy[0]} - {analogy[1]} + {analogy[2]} ≈ {result[0]}\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"movie_reviews.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[One, reviewers, mentioned, watching, 1, Oz, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[A, wonderful, little, production, br, br, The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[I, thought, wonderful, way, spend, time, hot,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Basically, theres, family, little, boy, Jake,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[Petter, Matteis, Love, Time, Money, visually,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  [One, reviewers, mentioned, watching, 1, Oz, e...  \n",
       "1  [A, wonderful, little, production, br, br, The...  \n",
       "2  [I, thought, wonderful, way, spend, time, hot,...  \n",
       "3  [Basically, theres, family, little, boy, Jake,...  \n",
       "4  [Petter, Matteis, Love, Time, Money, visually,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords_and_punctuation(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "df['cleaned_text'] = df['review'].apply(remove_stopwords_and_punctuation)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050317</td>\n",
       "      <td>0.053415</td>\n",
       "      <td>0.038032</td>\n",
       "      <td>0.076007</td>\n",
       "      <td>-0.053392</td>\n",
       "      <td>-0.002179</td>\n",
       "      <td>0.034439</td>\n",
       "      <td>-0.057433</td>\n",
       "      <td>0.076233</td>\n",
       "      <td>0.097119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065795</td>\n",
       "      <td>0.010038</td>\n",
       "      <td>-0.116087</td>\n",
       "      <td>0.017102</td>\n",
       "      <td>-0.040221</td>\n",
       "      <td>-0.023342</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>-0.075081</td>\n",
       "      <td>0.041535</td>\n",
       "      <td>-0.019835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066517</td>\n",
       "      <td>0.071847</td>\n",
       "      <td>-0.025412</td>\n",
       "      <td>0.046563</td>\n",
       "      <td>-0.047924</td>\n",
       "      <td>0.037785</td>\n",
       "      <td>0.033784</td>\n",
       "      <td>-0.072095</td>\n",
       "      <td>0.089296</td>\n",
       "      <td>0.087283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133700</td>\n",
       "      <td>0.024485</td>\n",
       "      <td>-0.059384</td>\n",
       "      <td>0.021904</td>\n",
       "      <td>-0.046019</td>\n",
       "      <td>-0.067627</td>\n",
       "      <td>0.066743</td>\n",
       "      <td>-0.064045</td>\n",
       "      <td>0.041706</td>\n",
       "      <td>-0.014796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.036720</td>\n",
       "      <td>0.048992</td>\n",
       "      <td>-0.009070</td>\n",
       "      <td>0.100696</td>\n",
       "      <td>-0.034379</td>\n",
       "      <td>-0.002581</td>\n",
       "      <td>0.046219</td>\n",
       "      <td>-0.048808</td>\n",
       "      <td>0.087476</td>\n",
       "      <td>0.094753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072960</td>\n",
       "      <td>0.044974</td>\n",
       "      <td>-0.114639</td>\n",
       "      <td>0.014727</td>\n",
       "      <td>-0.047793</td>\n",
       "      <td>-0.060343</td>\n",
       "      <td>0.034904</td>\n",
       "      <td>-0.066511</td>\n",
       "      <td>0.021394</td>\n",
       "      <td>-0.018467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068221</td>\n",
       "      <td>0.031535</td>\n",
       "      <td>-0.022251</td>\n",
       "      <td>0.091936</td>\n",
       "      <td>-0.045524</td>\n",
       "      <td>0.062335</td>\n",
       "      <td>0.051493</td>\n",
       "      <td>-0.050584</td>\n",
       "      <td>0.091021</td>\n",
       "      <td>0.099479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106683</td>\n",
       "      <td>0.033853</td>\n",
       "      <td>-0.139438</td>\n",
       "      <td>0.034197</td>\n",
       "      <td>-0.022073</td>\n",
       "      <td>-0.059132</td>\n",
       "      <td>-0.019083</td>\n",
       "      <td>-0.053927</td>\n",
       "      <td>0.019353</td>\n",
       "      <td>0.023621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.039528</td>\n",
       "      <td>0.033580</td>\n",
       "      <td>-0.006584</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>-0.020921</td>\n",
       "      <td>0.010489</td>\n",
       "      <td>0.040179</td>\n",
       "      <td>-0.054073</td>\n",
       "      <td>0.070070</td>\n",
       "      <td>0.041623</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108452</td>\n",
       "      <td>0.044189</td>\n",
       "      <td>-0.087717</td>\n",
       "      <td>0.020076</td>\n",
       "      <td>-0.043566</td>\n",
       "      <td>-0.031580</td>\n",
       "      <td>0.023033</td>\n",
       "      <td>-0.041160</td>\n",
       "      <td>0.047247</td>\n",
       "      <td>-0.022942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.050317  0.053415  0.038032  0.076007 -0.053392 -0.002179  0.034439   \n",
       "1  0.066517  0.071847 -0.025412  0.046563 -0.047924  0.037785  0.033784   \n",
       "2  0.036720  0.048992 -0.009070  0.100696 -0.034379 -0.002581  0.046219   \n",
       "3  0.068221  0.031535 -0.022251  0.091936 -0.045524  0.062335  0.051493   \n",
       "4  0.039528  0.033580 -0.006584  0.058266 -0.020921  0.010489  0.040179   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0 -0.057433  0.076233  0.097119  ... -0.065795  0.010038 -0.116087  0.017102   \n",
       "1 -0.072095  0.089296  0.087283  ... -0.133700  0.024485 -0.059384  0.021904   \n",
       "2 -0.048808  0.087476  0.094753  ... -0.072960  0.044974 -0.114639  0.014727   \n",
       "3 -0.050584  0.091021  0.099479  ... -0.106683  0.033853 -0.139438  0.034197   \n",
       "4 -0.054073  0.070070  0.041623  ... -0.108452  0.044189 -0.087717  0.020076   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0 -0.040221 -0.023342  0.002918 -0.075081  0.041535 -0.019835  \n",
       "1 -0.046019 -0.067627  0.066743 -0.064045  0.041706 -0.014796  \n",
       "2 -0.047793 -0.060343  0.034904 -0.066511  0.021394 -0.018467  \n",
       "3 -0.022073 -0.059132 -0.019083 -0.053927  0.019353  0.023621  \n",
       "4 -0.043566 -0.031580  0.023033 -0.041160  0.047247 -0.022942  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding(words):\n",
    "    embedding = [wv_pretrained[word] for word in words if word in wv_pretrained]\n",
    "    return np.mean(embedding, axis=0)\n",
    " \n",
    "embedding=df['cleaned_text'].apply(get_embedding)  \n",
    "\n",
    "data=pd.DataFrame(embedding.tolist())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.concat([df,data],axis=1)\n",
    "df1.drop(columns=['cleaned_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8511\n",
      "F1 Score: 0.8527055099416362\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df1['sentiment'] = le.fit_transform(df1['sentiment'])\n",
    "X = df1.drop(columns=['review','sentiment'])\n",
    "y = df1['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['cleaned_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    sg=1,\n",
    "    vector_size=50,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'wonderful': [ 0.03441551  0.39873645 -0.36233056 -0.02523674  0.02290742 -0.32009062\n",
      "  0.4691728   1.0497317  -0.87000555 -0.21361372 -0.07528584 -0.5301443\n",
      " -0.5063691  -0.16853376  0.05286307  0.22423881  0.7701386   0.11465402\n",
      " -0.66667956 -0.92209643  0.17873845  0.14453901  0.9097889  -0.77363205\n",
      "  0.4593626   0.2826991  -0.12487649  0.05664964 -0.03575842  0.09887584\n",
      "  0.4279547   0.00762904 -0.06376167  0.21833904 -0.5457569   0.02686579\n",
      "  0.44380885 -0.13114996 -0.11421734 -0.33974338  0.6559251  -0.15064533\n",
      " -0.6525172   0.3062481  -0.11914285 -0.13604423  0.0116007  -0.7699943\n",
      " -0.0292401  -0.08773963]\n"
     ]
    }
   ],
   "source": [
    "word_vector = skipgram_model.wv['wonderful']\n",
    "print(f\"Vector for 'wonderful': {word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model.save(\"skipgram_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = Word2Vec.load(\"skipgram_model.model\")\n",
    "loaded_model=loaded_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.063343</td>\n",
       "      <td>0.244839</td>\n",
       "      <td>-0.067982</td>\n",
       "      <td>0.060084</td>\n",
       "      <td>-0.073624</td>\n",
       "      <td>-0.163179</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>0.615599</td>\n",
       "      <td>-0.815102</td>\n",
       "      <td>-0.135929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336702</td>\n",
       "      <td>-0.117952</td>\n",
       "      <td>0.092497</td>\n",
       "      <td>-0.126463</td>\n",
       "      <td>0.743028</td>\n",
       "      <td>0.172608</td>\n",
       "      <td>-0.287929</td>\n",
       "      <td>-0.291094</td>\n",
       "      <td>0.174713</td>\n",
       "      <td>0.278893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.024392</td>\n",
       "      <td>0.182389</td>\n",
       "      <td>-0.190436</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>-0.013198</td>\n",
       "      <td>-0.127765</td>\n",
       "      <td>0.310726</td>\n",
       "      <td>0.711857</td>\n",
       "      <td>-0.771701</td>\n",
       "      <td>-0.077946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477269</td>\n",
       "      <td>-0.084128</td>\n",
       "      <td>0.022730</td>\n",
       "      <td>0.027071</td>\n",
       "      <td>0.610105</td>\n",
       "      <td>0.214239</td>\n",
       "      <td>-0.270501</td>\n",
       "      <td>-0.360278</td>\n",
       "      <td>0.157368</td>\n",
       "      <td>0.226567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.016882</td>\n",
       "      <td>0.198719</td>\n",
       "      <td>-0.052379</td>\n",
       "      <td>-0.007732</td>\n",
       "      <td>-0.017361</td>\n",
       "      <td>-0.166297</td>\n",
       "      <td>0.457060</td>\n",
       "      <td>0.694028</td>\n",
       "      <td>-0.857787</td>\n",
       "      <td>-0.102636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328677</td>\n",
       "      <td>-0.170674</td>\n",
       "      <td>0.056612</td>\n",
       "      <td>-0.063626</td>\n",
       "      <td>0.601959</td>\n",
       "      <td>0.167699</td>\n",
       "      <td>-0.355524</td>\n",
       "      <td>-0.404060</td>\n",
       "      <td>0.149560</td>\n",
       "      <td>0.238432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.066131</td>\n",
       "      <td>0.338758</td>\n",
       "      <td>-0.102425</td>\n",
       "      <td>0.079144</td>\n",
       "      <td>-0.067801</td>\n",
       "      <td>-0.195056</td>\n",
       "      <td>0.429991</td>\n",
       "      <td>0.643728</td>\n",
       "      <td>-0.826050</td>\n",
       "      <td>-0.125206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348844</td>\n",
       "      <td>-0.225357</td>\n",
       "      <td>0.070194</td>\n",
       "      <td>-0.077345</td>\n",
       "      <td>0.750696</td>\n",
       "      <td>0.202124</td>\n",
       "      <td>-0.331156</td>\n",
       "      <td>-0.364595</td>\n",
       "      <td>0.164018</td>\n",
       "      <td>0.219410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.037951</td>\n",
       "      <td>0.254834</td>\n",
       "      <td>-0.031797</td>\n",
       "      <td>0.021243</td>\n",
       "      <td>-0.034110</td>\n",
       "      <td>-0.236147</td>\n",
       "      <td>0.393718</td>\n",
       "      <td>0.707003</td>\n",
       "      <td>-0.796103</td>\n",
       "      <td>-0.104750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389474</td>\n",
       "      <td>-0.130841</td>\n",
       "      <td>0.086422</td>\n",
       "      <td>-0.038045</td>\n",
       "      <td>0.558677</td>\n",
       "      <td>0.231148</td>\n",
       "      <td>-0.183375</td>\n",
       "      <td>-0.348792</td>\n",
       "      <td>0.174064</td>\n",
       "      <td>0.197762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.063343  0.244839 -0.067982  0.060084 -0.073624 -0.163179  0.426667   \n",
       "1 -0.024392  0.182389 -0.190436  0.038050 -0.013198 -0.127765  0.310726   \n",
       "2 -0.016882  0.198719 -0.052379 -0.007732 -0.017361 -0.166297  0.457060   \n",
       "3 -0.066131  0.338758 -0.102425  0.079144 -0.067801 -0.195056  0.429991   \n",
       "4 -0.037951  0.254834 -0.031797  0.021243 -0.034110 -0.236147  0.393718   \n",
       "\n",
       "         7         8         9   ...        40        41        42        43  \\\n",
       "0  0.615599 -0.815102 -0.135929  ...  0.336702 -0.117952  0.092497 -0.126463   \n",
       "1  0.711857 -0.771701 -0.077946  ...  0.477269 -0.084128  0.022730  0.027071   \n",
       "2  0.694028 -0.857787 -0.102636  ...  0.328677 -0.170674  0.056612 -0.063626   \n",
       "3  0.643728 -0.826050 -0.125206  ...  0.348844 -0.225357  0.070194 -0.077345   \n",
       "4  0.707003 -0.796103 -0.104750  ...  0.389474 -0.130841  0.086422 -0.038045   \n",
       "\n",
       "         44        45        46        47        48        49  \n",
       "0  0.743028  0.172608 -0.287929 -0.291094  0.174713  0.278893  \n",
       "1  0.610105  0.214239 -0.270501 -0.360278  0.157368  0.226567  \n",
       "2  0.601959  0.167699 -0.355524 -0.404060  0.149560  0.238432  \n",
       "3  0.750696  0.202124 -0.331156 -0.364595  0.164018  0.219410  \n",
       "4  0.558677  0.231148 -0.183375 -0.348792  0.174064  0.197762  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def skip_embedding(words):\n",
    "    embedding = [loaded_model[word] for word in words if word in loaded_model]\n",
    "    return np.mean(embedding, axis=0)\n",
    " \n",
    "embedding = df['cleaned_text'].apply(skip_embedding)  \n",
    " \n",
    " \n",
    "data=pd.DataFrame(embedding.tolist())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.concat([df,data],axis=1)\n",
    "df2.drop(columns=['cleaned_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8655\n",
      "F1 Score: 0.8670554512207176\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df2['sentiment'] = le.fit_transform(df2['sentiment'])\n",
    "X = df2.drop(columns=['review','sentiment'])\n",
    "y = df2['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    sg=0,\n",
    "    vector_size=50,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model.save(\"cbow_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = Word2Vec.load(\"cbow_model.model\")\n",
    "loaded_model=cbow_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'wonderful': [-0.81846875 -0.9106931  -2.3695414  -2.097507    0.21988308 -1.7993802\n",
      "  2.1262617   1.2597257  -0.5110926  -2.1595979  -1.1689672  -1.8074325\n",
      " -1.5526415   0.7066251   1.4114614   0.72903687  1.7051877   2.7015388\n",
      " -0.81129    -0.30341464 -2.116086   -1.7197975   1.390849   -2.229772\n",
      " -3.05413     2.3331795   2.5287955   0.07866892  2.693956    2.149588\n",
      "  0.4201168   2.025552    1.3107519   2.9648268  -0.74227196  1.06344\n",
      "  2.2967203  -2.003037    1.4405121  -1.1280355   2.6575353  -0.04896227\n",
      " -3.2848303   1.1505247  -0.6700001  -2.3484464   1.965812   -1.2625608\n",
      " -1.7398496  -0.8539397 ]\n"
     ]
    }
   ],
   "source": [
    "word_vector = cbow_model.wv['wonderful']\n",
    "print(f\"Vector for 'wonderful': {word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.009375</td>\n",
       "      <td>-0.083878</td>\n",
       "      <td>0.129996</td>\n",
       "      <td>0.190679</td>\n",
       "      <td>-0.227386</td>\n",
       "      <td>-0.037609</td>\n",
       "      <td>0.015466</td>\n",
       "      <td>1.251132</td>\n",
       "      <td>-0.772087</td>\n",
       "      <td>-0.464090</td>\n",
       "      <td>...</td>\n",
       "      <td>1.001043</td>\n",
       "      <td>0.352904</td>\n",
       "      <td>0.214728</td>\n",
       "      <td>-0.456391</td>\n",
       "      <td>1.062356</td>\n",
       "      <td>-0.306075</td>\n",
       "      <td>0.248367</td>\n",
       "      <td>-0.412648</td>\n",
       "      <td>-0.145744</td>\n",
       "      <td>0.441690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.429080</td>\n",
       "      <td>-0.300830</td>\n",
       "      <td>-0.424902</td>\n",
       "      <td>0.210352</td>\n",
       "      <td>-0.537999</td>\n",
       "      <td>-0.112745</td>\n",
       "      <td>0.222454</td>\n",
       "      <td>0.838348</td>\n",
       "      <td>-0.655557</td>\n",
       "      <td>-0.321907</td>\n",
       "      <td>...</td>\n",
       "      <td>1.780913</td>\n",
       "      <td>-0.021433</td>\n",
       "      <td>-0.293082</td>\n",
       "      <td>0.034497</td>\n",
       "      <td>0.879431</td>\n",
       "      <td>-0.125925</td>\n",
       "      <td>0.680777</td>\n",
       "      <td>-0.777004</td>\n",
       "      <td>-0.667192</td>\n",
       "      <td>0.457551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.027704</td>\n",
       "      <td>-0.145170</td>\n",
       "      <td>-0.051468</td>\n",
       "      <td>0.036498</td>\n",
       "      <td>-0.435205</td>\n",
       "      <td>-0.130171</td>\n",
       "      <td>0.443805</td>\n",
       "      <td>1.297957</td>\n",
       "      <td>-0.611076</td>\n",
       "      <td>-0.516509</td>\n",
       "      <td>...</td>\n",
       "      <td>1.636828</td>\n",
       "      <td>0.180638</td>\n",
       "      <td>0.389332</td>\n",
       "      <td>-0.413120</td>\n",
       "      <td>0.601427</td>\n",
       "      <td>-0.601080</td>\n",
       "      <td>0.362092</td>\n",
       "      <td>-0.735718</td>\n",
       "      <td>-0.520859</td>\n",
       "      <td>0.209026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.041352</td>\n",
       "      <td>0.026649</td>\n",
       "      <td>0.125734</td>\n",
       "      <td>0.032092</td>\n",
       "      <td>-0.056897</td>\n",
       "      <td>-0.194724</td>\n",
       "      <td>0.402916</td>\n",
       "      <td>0.898428</td>\n",
       "      <td>-0.599401</td>\n",
       "      <td>-0.397633</td>\n",
       "      <td>...</td>\n",
       "      <td>1.668607</td>\n",
       "      <td>-0.232929</td>\n",
       "      <td>0.333019</td>\n",
       "      <td>-0.748645</td>\n",
       "      <td>1.197575</td>\n",
       "      <td>-0.456281</td>\n",
       "      <td>0.755339</td>\n",
       "      <td>-1.138464</td>\n",
       "      <td>-0.352200</td>\n",
       "      <td>0.395871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.661996</td>\n",
       "      <td>-0.455954</td>\n",
       "      <td>0.045143</td>\n",
       "      <td>-0.236234</td>\n",
       "      <td>-0.483727</td>\n",
       "      <td>-0.391741</td>\n",
       "      <td>-0.042647</td>\n",
       "      <td>1.019380</td>\n",
       "      <td>-0.565574</td>\n",
       "      <td>-0.569736</td>\n",
       "      <td>...</td>\n",
       "      <td>1.673474</td>\n",
       "      <td>0.069063</td>\n",
       "      <td>0.308174</td>\n",
       "      <td>-0.247994</td>\n",
       "      <td>0.665041</td>\n",
       "      <td>-0.140750</td>\n",
       "      <td>0.903648</td>\n",
       "      <td>-0.702217</td>\n",
       "      <td>-0.507646</td>\n",
       "      <td>0.281876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.009375 -0.083878  0.129996  0.190679 -0.227386 -0.037609  0.015466   \n",
       "1 -0.429080 -0.300830 -0.424902  0.210352 -0.537999 -0.112745  0.222454   \n",
       "2 -0.027704 -0.145170 -0.051468  0.036498 -0.435205 -0.130171  0.443805   \n",
       "3 -0.041352  0.026649  0.125734  0.032092 -0.056897 -0.194724  0.402916   \n",
       "4 -0.661996 -0.455954  0.045143 -0.236234 -0.483727 -0.391741 -0.042647   \n",
       "\n",
       "         7         8         9   ...        40        41        42        43  \\\n",
       "0  1.251132 -0.772087 -0.464090  ...  1.001043  0.352904  0.214728 -0.456391   \n",
       "1  0.838348 -0.655557 -0.321907  ...  1.780913 -0.021433 -0.293082  0.034497   \n",
       "2  1.297957 -0.611076 -0.516509  ...  1.636828  0.180638  0.389332 -0.413120   \n",
       "3  0.898428 -0.599401 -0.397633  ...  1.668607 -0.232929  0.333019 -0.748645   \n",
       "4  1.019380 -0.565574 -0.569736  ...  1.673474  0.069063  0.308174 -0.247994   \n",
       "\n",
       "         44        45        46        47        48        49  \n",
       "0  1.062356 -0.306075  0.248367 -0.412648 -0.145744  0.441690  \n",
       "1  0.879431 -0.125925  0.680777 -0.777004 -0.667192  0.457551  \n",
       "2  0.601427 -0.601080  0.362092 -0.735718 -0.520859  0.209026  \n",
       "3  1.197575 -0.456281  0.755339 -1.138464 -0.352200  0.395871  \n",
       "4  0.665041 -0.140750  0.903648 -0.702217 -0.507646  0.281876  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cbow_embedding(words):\n",
    "    embedding = [loaded_model[word] for word in words if word in loaded_model]\n",
    "    return np.mean(embedding, axis=0)\n",
    " \n",
    "embedding = df['cleaned_text'].apply(cbow_embedding)  \n",
    " \n",
    "data=pd.DataFrame(embedding.tolist())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.concat([df,data],axis=1)\n",
    "df3.drop(columns=['cleaned_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.842\n",
      "F1 Score: 0.8439660280466127\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df3['sentiment'] = le.fit_transform(df3['sentiment'])\n",
    "X = df3.drop(columns=['review','sentiment'])\n",
    "y = df3['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
